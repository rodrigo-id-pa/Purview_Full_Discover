{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS: \n",
    "# Precisa do azure configurado\n",
    "# Precisa do purview configurado no azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "from pyapacheatlas.auth import ServicePrincipalAuthentication\n",
    "from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess, TypeCategory\n",
    "from pyapacheatlas.core.typedef import *\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para autenticar a entidade de serviço para o URL de recurso fornecido e retorna o token oauth2 de acesso\n",
    "def azuread_auth(tenant_id: str, client_id: str, client_secret: str, resource_url: str):\n",
    "    \n",
    "    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "    payload= f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource={resource_url}'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    access_token = json.loads(response.text)['access_token']\n",
    "    \n",
    "    return access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para autenticar no Atlas Endpoint e retorna um objeto Client\n",
    "def purview_auth(tenant_id: str, client_id: str, client_secret: str, data_catalog_name: str):\n",
    "  \n",
    "    oauth2 = ServicePrincipalAuthentication(\n",
    "        tenant_id = tenant_id,\n",
    "        client_id = client_id,\n",
    "        client_secret = client_secret\n",
    "    )\n",
    "    client = PurviewClient(\n",
    "        account_name = data_catalog_name,\n",
    "        authentication = oauth2\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para utilizar no Jupyter\n",
    "\n",
    "tenant_id = \"\"\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "resource_url = \"https://purview.azure.net\"\n",
    "data_catalog_name = \"\"\n",
    "\n",
    "#pegando data atual e formatando\n",
    "data_today = datetime.datetime.today() - timedelta(hours=3, minutes=0)\n",
    "data_now_str = data_today.strftime(\"%A %d %B %y %H:%M\")\n",
    "date_now = datetime.datetime.strptime(data_now_str, \"%A %d %B %y %H:%M\")\n",
    "\n",
    "## OBS: REMOVA Todo CONTEÚDO DO EX: SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para utilizar com Spark\n",
    "\n",
    "# Abrir conexão com o datalake\n",
    "spark.conf.set(\n",
    "  \"\", # Endpoint do datalake\n",
    "  \"\" # Acess Key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar json com as chaves de acesso do azure e purview\n",
    "\n",
    "# Recuperar caminho até o json de parâmetros da Autentificação do Purview\n",
    "path_params = dbutils.fs.ls('''path do json no azure''')\n",
    "\n",
    "# Recuperar dados do json de parâmetros da Autentificação do Purview\n",
    "parameters = spark.read.format(\"json\").load(path_params)\n",
    "\n",
    "client_id = parameters.select('client_id').collect()[0][0]\n",
    "client_secret = parameters.select('client_secret').collect()[0][0]\n",
    "data_catalog_name = parameters.select('data_catalog_name').collect()[0][0]\n",
    "resource_url = parameters.select('resource_url').collect()[0][0]\n",
    "tenant_id = parameters.select('tenant_id').collect()[0][0]\n",
    "\n",
    "#pegando data atual e formatando\n",
    "data_today = datetime.datetime.today() - timedelta(hours=3, minutes=0)\n",
    "data_now_str = data_today.strftime(\"%A %d %B %y %H:%M\")\n",
    "date_now = datetime.datetime.strptime(data_now_str, \"%A %d %B %y %H:%M\")\n",
    "\n",
    "# Recuperar objetos de autenticação\n",
    "azuread_access_token = azuread_auth(tenant_id, client_id, client_secret, resource_url)\n",
    "purview_client = purview_auth(tenant_id, client_id, client_secret, data_catalog_name)\n",
    "\n",
    "# OBS: REMOVA TODO O CONTEÚDO DO EX: JUPYTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para fazer requisição de dados de acordo com a coleção no Purview\n",
    "def requestCollection(endpoint: str, url: str, response: any, payload: any, headers: any):\n",
    "    endpoint = endpoint\n",
    "    url = url\n",
    "    payload = payload\n",
    "    headers = headers\n",
    "    response = response\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## função para fazer requisição de dados classificados\n",
    "def requestId(collectionId: str):\n",
    "    endpoint = \"https://\"+data_catalog_name+\".purview.azure.com/\"\n",
    "    url = f\"{endpoint}/catalog/api/search/query?api-version=2021-05-01-preview\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {azuread_access_token}'\n",
    "       }\n",
    "    payload = json.dumps({\n",
    "            \"orderby\":[\"name\"],\n",
    "            \"limit\": 1000,\n",
    "            \"keywords\": None,\n",
    "            \"filter\": {\n",
    "                \"and\": [\n",
    "                        {\n",
    "                    \"or\": [\n",
    "                      {\n",
    "                        \"collectionId\": collectionId\n",
    "                      },\n",
    "                    ]\n",
    "                  },\n",
    "                  {\n",
    "                    \"or\": [\n",
    "                      {\n",
    "                        \"classification\": \"sua classificação\",\n",
    "                        \"includeSubClassifications\": True\n",
    "                      },\n",
    "                      {\n",
    "                        \"classification\": \"sua classificação\",\n",
    "                        \"includeSubClassifications\": True\n",
    "                      },\n",
    "                      {\n",
    "                        \"classification\": \"MICROSOFT.SYSTEM.TEMP_FILE\",\n",
    "                        \"includeSubClassifications\": True\n",
    "                      }\n",
    "                    ]\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "    })\n",
    "    \n",
    "    response = requests.post(url, headers=headers, data=payload)\n",
    "    \n",
    "    if(response.status_code != 200):\n",
    "      print(\"Status:\",response.status_code, \"Erro no código\")\n",
    "    else:\n",
    "      print(\"Status:\",response.status_code, \"Ok\")\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para exportar na pasta temporária databricks e depois mover para pasta no azure\n",
    "def export_file(dataframe, ext_target, target, new_name):\n",
    "  df_res = dataframe\n",
    "  if ext_target == 'csv':\n",
    "    df_res.to_csv('/dbfs/tmp/meuCaminho/'+new_name+'.'+ext_target, sep= ';', index=False)\n",
    "  if ext_target == 'parquet':\n",
    "    df_res.to_parquet('/dbfs/tmp/meuCaminho/'+new_name+'.'+ext_target, index=False)\n",
    "  dbutils.fs.mv(\"/tmp/purview/\"+new_name+'.'+ext_target, target+'/'+new_name+'.'+ext_target)\n",
    "  print ('Novo arquivo {} salvo em {}'.format(new_name, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trazendo todos os dados das coleções do Purview\n",
    "endpoint = \"https://\"+data_catalog_name+\".purview.azure.com/\"\n",
    "url = f\"{endpoint}/account/collections?api-version=2019-11-01-preview\"\n",
    "headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {azuread_access_token}'\n",
    "       }\n",
    "\n",
    "payload = azuread_access_token\n",
    "\n",
    "response = requests.get(url, headers=headers, data=payload)\n",
    "\n",
    "if(response.status_code != 200):\n",
    "    print(\"Status:\",response.status_code, \"Erro no código\", response.text)\n",
    "else:\n",
    "    print(\"Status:\",response.status_code, \"Ok\", response.text)\n",
    "    \n",
    "requestCollection(endpoint, url, headers, payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe com todos os dados\n",
    "df = pd.DataFrame(json.loads(response.text)['value'])\n",
    "\n",
    "# listando somente as coleções que vamos utilizar\n",
    "list_colection = ((df['friendlyName'] == 'nomeColeção') | (df['friendlyName'] == 'nomeColeção') | (df['friendlyName'] == 'nomeColeção'))\n",
    "df_colection = df[list_colection]\n",
    "df_colection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## requisição em cada coleção para trazer os dados\n",
    "list_collections = df_colection.loc[:, 'name']\n",
    "\n",
    "dict_results = {}\n",
    "\n",
    "for list_db in list_collections:    \n",
    "    db_result = requestId(list_db)\n",
    "    dict_results['db_id_'+str(list_db)] = db_result\n",
    "    \n",
    "list_results = list(dict_results.values())\n",
    "\n",
    "## dataframe com as tabelas de cada coleção\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "for x in list_results:\n",
    "    df_value = pd.DataFrame(json.loads(x)['value'])\n",
    "    df_results = pd.concat([df_results, df_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Todas as tabelas e colunas das coleções\n",
    "list_response = []\n",
    "for id_ in df_results['id']:\n",
    "    endpoint = \"https://\"+data_catalog_name+\".purview.azure.com/\"\n",
    "    url = f\"{endpoint}/catalog/api/atlas/v2/entity/bulk?excludeRelationshipTypes=dataset_process_inputs&excludeRelationshipTypes=process_parent&excludeRelationshipTypes=direct_lineage_dataset_dataset&guid={id_}&includeTermsInMinExtInfo=false&minExtInfo=false&ignoreRelationships=false\"\n",
    "    headers = {\n",
    "              'Content-Type': 'application/json',\n",
    "              'Authorization': f'Bearer {azuread_access_token}'\n",
    "             }\n",
    "    payload = azuread_access_token\n",
    "\n",
    "    response = requests.get(url, headers=headers, data=payload)\n",
    "    requestCollection(endpoint, url, response, headers, payload)\n",
    "    list_response.append(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe com todos os dados\n",
    "df_list_ = pd.DataFrame(list_response)\n",
    "\n",
    "## percorrendo e listando cada coluna no dataframe para buscar nomes e classificações\n",
    "df_list_res_referredEntities = pd.DataFrame()\n",
    "for x in list_response:\n",
    "    df_list = pd.DataFrame(json.loads(x)['referredEntities']).reindex(['collectionId','attributes','classifications']).transpose().dropna()\n",
    "    df_list_res_referredEntities = pd.concat([df_list_res_referredEntities, df_list])\n",
    "\n",
    "df_list_res_referredEntities.reset_index(inplace=True)\n",
    "df_list_res_referredEntities = df_list_res_referredEntities.rename(columns = {'index':'IDs'})\n",
    "\n",
    "## listando dataframe com nome das colunas e criando nova coluna para filtrar as classificações\n",
    "df_columns = df_list_res_referredEntities\n",
    "df_columns['classificação'] = ''\n",
    "df_columns['data'] = date_now\n",
    "\n",
    "for i in range(df_columns.shape[0]):\n",
    "    list_=[]\n",
    "    df_columns.loc[i,'attributes'] = df_columns.loc[i,'attributes']['name']\n",
    "    list_.append([item_['typeName'] for item_ in df_columns.classifications[i]])\n",
    "    df_columns.loc[:,'classificação'].iloc[i] = list_[0]  \n",
    "\n",
    "## dataframe com colunas e suas classificações\n",
    "df_col = df_columns.loc[:, ['IDs','collectionId','attributes', 'classificação', 'data']]\n",
    "\n",
    "## dataframe com tabelas e suas classificações\n",
    "df_tables = df_results.loc[:, ['id','collectionId','name', 'classification', 'qualifiedName']]\n",
    "df_tables.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe com nome das tabelas e filtrando as classificações para nova tabela\n",
    "df_tables['data2'] = date_now\n",
    "\n",
    "## dataframe com tabelas e suas classificações\n",
    "df_tab = df_tables.loc[:, ['id', 'collectionId', 'name', 'classification', 'data2', 'qualifiedName']]\n",
    "\n",
    "# Igualando os IDs das tabelas com os das colunas\n",
    "df_tab.loc[:,'Id_simplificado'] = ''\n",
    "for count, item in enumerate(df_tab.id):\n",
    "    df_tab.loc[:,'Id_simplificado'].iloc[count] = item[:33]\n",
    "    \n",
    "df_col.loc[:,'Id_simplificado'] = ''    \n",
    "for count, item in enumerate(df_col.IDs):\n",
    "    df_col.loc[:,'Id_simplificado'].iloc[count] = item[:33]\n",
    "\n",
    "# Tratando a URL dos bancos para fazer o agrupamento  \n",
    "df_tab.loc[:,'Owner'] = ''    \n",
    "for count, item in enumerate(df_tab.qualifiedName):\n",
    "    df_tab.loc[:,'Owner'].iloc[count] = item[:47]  \n",
    "\n",
    "## dataframe final com todas as tabelas e suas colunas com id, nome e classificação\n",
    "tb_merge = df_tab.merge(df_col, left_on=['Id_simplificado', 'collectionId'], right_on=['Id_simplificado', 'collectionId'])\n",
    "\n",
    "## dataframe final com tabelas e colunas classificadas\n",
    "df = tb_merge[['collectionId', 'name', 'classification', 'attributes', 'classificação', 'data', 'Owner']]\\\n",
    ".rename(columns={'name':'vc_nm_tabela', \n",
    "                 'classification':'vc_nm_grupoclassificacao', \n",
    "                 'attributes':'vc_nm_coluna', \n",
    "                 'classificação':'vc_ds_classificacao', \n",
    "                 'data': 'sd_dt_carga',\n",
    "                 'Owner': 'vc_nm_owner'\n",
    "                }\n",
    "       )\n",
    "\n",
    "#convertendo data para string\n",
    "df.sd_dt_carga = df.sd_dt_carga.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupando os dados de acordo com a coleção\n",
    "grouped_df = df.groupby('vc_nm_owner')\n",
    "\n",
    "df_1 = grouped_df.get_group('mssql://path_do_banco_purview').drop(['collectionId', 'vc_nm_owner'], axis=1)\n",
    "\n",
    "df_2 = grouped_df.get_group('mssql://path_do_banco_purview').drop(['collectionId', 'vc_nm_owner'], axis=1)\n",
    "\n",
    "df_3 = grouped_df.get_group('mssql://path_do_banco_purview').drop(['collectionId', 'vc_nm_owner'], axis=1)\n",
    "\n",
    "df_4 = grouped_df.get_group('mssql://path_do_banco_purview').drop(['collectionId', 'vc_nm_owner'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo parquet ou csv no container landing\n",
    "df_list_.columns = df_list_.columns.astype(str)\n",
    "dataframe = df_list_\n",
    "ext_target=\"parquet\"\n",
    "path_raw = 'path_landing do azure'\n",
    "new_name = f\"nome_do_arquivo\"\n",
    "export_file(dataframe, ext_target, path_raw, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exportando os df csv ou parquet no container raw\n",
    "# Defina os dados e os nomes dos arquivos em uma lista\n",
    "datasets = [\n",
    "    (df_1, 'df_1'),\n",
    "    (df_2, 'df_2'),\n",
    "    (df_3, 'df_3'),\n",
    "    (df_4, 'df_4')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop através dos datasets\n",
    "for dataset, filename in datasets:\n",
    "    dataset.reset_index(inplace=True, drop=True)\n",
    "    dataset['vc_nm_grupoclassificacao'] = dataset['vc_nm_grupoclassificacao'].map(str)\n",
    "    dataset['vc_ds_classificacao'] = dataset['vc_ds_classificacao'].map(str)\n",
    "\n",
    "    # Criar arquivo parquet ou csv no container raw\n",
    "    dataframe = dataset\n",
    "    ext_target = \"parquet\"\n",
    "    path_raw = 'path_raw do azure'\n",
    "    new_name = f\"{filename}\"\n",
    "    export_file(dataframe, ext_target, path_raw + filename, new_name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
